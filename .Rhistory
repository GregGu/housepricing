ntrees = 10000,
learn_rate = .005,
max_depth = 4)
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
base_models = list(gbmfin, glmfin))
pred <- exp(h2o.predict(object=gbmfin, newdata=test))
pred <- exp(h2o.predict(object=gbmfin, newdata=test))
pred
dftest <- read.csv(here::here("data-raw", "test.csv"))
dftest <- read.csv(here::here("data-raw", "test.csv"))
final <- data.frame(Id = dftest %>% dplyr::pull(Id), SalePrice = as.vector(pred))
write.csv(final, row.names = FALSE, "predictions.csv")
y <- "SalePrice"
x <- setdiff(names(train), c(y, "fold"))
gbmfin <- h2o.gbm(x = x, y = y,
training_frame = train, seed=1,
ntrees = 16000,
learn_rate = .005,
max_depth = 5)
pred <- exp(h2o.predict(object=gbmfin, newdata=test))
pred <- exp(h2o.predict(object=gbmfin, newdata=test))
pred
dftest <- read.csv(here::here("data-raw", "test.csv"))
final <- data.frame(Id = dftest %>% dplyr::pull(Id), SalePrice = as.vector(pred))
write.csv(final, row.names = FALSE, "predictions.csv")
y <- "SalePrice"
x <- setdiff(names(train), c(y, "fold"))
gbmfin <- h2o.gbm(x = x, y = y,
training_frame = train, seed=1,
ntrees = 16000,
learn_rate = .005,
max_depth = 4,
col_sample_rate = .7)
pred <- exp(h2o.predict(object=gbmfin, newdata=test))
pred <- exp(h2o.predict(object=gbmfin, newdata=test))
pred
dftest <- read.csv(here::here("data-raw", "test.csv"))
final <- data.frame(Id = dftest %>% dplyr::pull(Id), SalePrice = as.vector(pred))
write.csv(final, row.names = FALSE, "predictions.csv")
y <- "SalePrice"
x <- setdiff(names(train), c(y))#, "fold"))
glmfin <- h2o.glm(x = x, y = y,
training_frame = train, seed=1,
fold_column = "fold",
keep_cross_validation_predictions = TRUE,
alpha = .5,
lambda = 0)
y <- "SalePrice"
x <- setdiff(names(train), c(y))#, "fold"))
gbmfin <- h2o.gbm(x = x, y = y,
training_frame = train, seed=1,
fold_column = "fold",
keep_cross_validation_predictions = TRUE,
ntrees = 16000,
learn_rate = .005,
max_depth = 5)
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
base_models = list(gbmfin, glmfin))
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
base_models = list(gbmfin, glmfin))
pred <- exp(h2o.predict(object=ensemble, newdata=test))
pred
dftest <- read.csv(here::here("data-raw", "test.csv"))
y <- "SalePrice"
x <- setdiff(names(train), c(y, "fold"))
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
base_models = list(gbmfin, glmfin))
pred <- exp(h2o.predict(object=ensemble, newdata=test))
pred <- exp(h2o.predict(object=ensemble, newdata=test))
pred
dftest <- read.csv(here::here("data-raw", "test.csv"))
final <- data.frame(Id = dftest %>% dplyr::pull(Id), SalePrice = as.vector(pred))
write.csv(final, row.names = FALSE, "predictions.csv")
y <- "SalePrice"
x <- setdiff(names(train), c(y, "fold"))
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
base_models = list(gbmfin, glmfin))
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train %>% dplyr::select(-fold),
base_models = list(gbmfin, glmfin))
pred <- exp(h2o.predict(object=ensemble, newdata=test))
test$fold <- NA
test$fold <- NULL
pred <- exp(h2o.predict(object=ensemble, newdata=test))
?h2o.predict
y <- "SalePrice"
x <- setdiff(names(train), c(y, "fold"))
glmfin <- h2o.glm(x = x, y = y,
training_frame = train, seed=1,
fold_column = "fold",
keep_cross_validation_predictions = TRUE,
alpha = .5,
lambda = 0)
y <- "SalePrice"
x <- setdiff(names(train), c(y, "fold"))
gbmfin <- h2o.gbm(x = x, y = y,
training_frame = train, seed=1,
fold_column = "fold",
keep_cross_validation_predictions = TRUE,
ntrees = 16000,
learn_rate = .005,
max_depth = 5)
y <- "SalePrice"
x <- setdiff(names(train), c(y, "fold"))
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
base_models = list(gbmfin, glmfin))
pred <- exp(h2o.predict(object=ensemble, newdata=test))
pred
dftest <- read.csv(here::here("data-raw", "test.csv"))
final <- data.frame(Id = dftest %>% dplyr::pull(Id), SalePrice = as.vector(pred))
write.csv(final, row.names = FALSE, "predictions.csv")
train2 <- train
train2$fold <- NULL
ensemble2 <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
base_models = list(gbmfin, glmfin))
pred <- exp(h2o.predict(object=ensemble2, newdata=test))
library(magrittr)
df <- read.csv("dffinal.csv")
too_low_var <-
c(
"Condition2",
"Condition1",
"Alley",
"Fence",
"BsmtCond",
"Electrical",
"LandSlope",
"ExterCond",
"Functional",
"Neighborhood2")
# one hot encode
df <- df %>%
dplyr::select(-too_low_var)
# one hot encode
nomvars <- names(df)[sapply(df, class) %in% c('factor',"character")]
df2 <- df %>%
dplyr::mutate_at(nomvars, as.factor)%>%
dplyr::select(nomvars) %>%
dplyr::select(-set)
library(caret)
dummy <- caret::dummyVars(" ~ .", data=df2, fullRank = FALSE)
df2 <- data.frame(predict(dummy, newdata = df2))
dff <- cbind(df2, df %>% dplyr::select(-nomvars))
dff$set <- df$set
train <- dff %>%
dplyr::filter(set == "train") %>%
dplyr::select(-set)
test <- dff %>%
dplyr::filter(set == "test") %>%
dplyr::select(-set)
library(h2o)
h2o.init(nthreads= -1, max_mem_size = "8g")
train <- as.h2o(train)
test <- as.h2o(test)
seed <- 12345
# For k_fold strategy we need to provide fold column
# train$fold <- h2o.kfold_column(data = train, nfolds = 10, seed = seed)
# hyper_params <- list( lambda = c(1, 0.5, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0) ,
#                        alpha = c(0, .25, .5,.75,.89,.99,1))
#
# y <- "SalePrice"
# x <- setdiff(names(train), y)
# # Grid search for selecting the best model
# grid <- h2o.grid(x = x, y = y ,
#                  training_frame = train,
#                  fold_column = "fold",
#                  algorithm = "glm", grid_id = "id6", hyper_params = hyper_params,seed=1,
#                  search_criteria = list(strategy = "Cartesian"))
# grid
# sortedGrid <- h2o.getGrid("id6", sort_by = "rmse", decreasing = FALSE)
# sortedGrid
y <- "SalePrice"
x <- setdiff(names(train), c(y, "fold"))
glmfin <- h2o.glm(x = x, y = y,
training_frame = train, seed=1,
nfolds = 5,
keep_cross_validation_predictions = TRUE,
alpha = .5,
lambda = 0)
# hyper_params <- list(ntrees = 300,
#                      learn_rate = c(0.05,0.005),
#                      max_depth  = c(2:5))
# hyper_params <- list(ntrees = 10000,
#                      learn_rate = c(0.005),
#                      max_depth  = c(4:5))
# y <- "SalePrice"
# x <- setdiff(names(train), y)
# # Grid search for selecting the best model
# grid <- h2o.grid(x = x, y = y ,
#                  training_frame = train,
#                  fold_column = "fold",
#                  algorithm = "gbm", grid_id = "id7", hyper_params = hyper_params,seed=1,
#                  search_criteria = list(strategy = "Cartesian"))
# grid
# sortedGrid <- h2o.getGrid("id7", sort_by = "rmse", decreasing = FALSE)
# sortedGrid
y <- "SalePrice"
x <- setdiff(names(train), c(y, "fold"))
gbmfin <- h2o.gbm(x = x, y = y,
training_frame = train, seed=1,
nfolds = 5,
keep_cross_validation_predictions = TRUE,
ntrees = 16000,
learn_rate = .005,
max_depth = 5)
# pred <- exp(h2o.predict(object=gbmfin, newdata=test))
# pred
# dftest <- read.csv(here::here("data-raw", "test.csv"))
#
# final <- data.frame(Id = dftest %>% dplyr::pull(Id), SalePrice = as.vector(pred))
# write.csv(final, row.names = FALSE, "predictions.csv")
y <- "SalePrice"
x <- setdiff(names(train), c(y, "fold"))
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
base_models = list(gbmfin, glmfin))
pred <- exp(h2o.predict(object=ensemble, newdata=test))
pred
dftest <- read.csv(here::here("data-raw", "test.csv"))
final <- data.frame(Id = dftest %>% dplyr::pull(Id), SalePrice = as.vector(pred))
write.csv(final, row.names = FALSE, "predictions.csv")
y <- "SalePrice"
x <- setdiff(names(train), c(y, "fold"))
glmfin <- h2o.glm(x = x, y = y,
training_frame = train, seed=1,
alpha = .25,
lambda = 0)
pred <- exp(h2o.predict(object=glmfin, newdata=test))
pred
dftest <- read.csv(here::here("data-raw", "test.csv"))
final <- data.frame(Id = dftest %>% dplyr::pull(Id), SalePrice = as.vector(pred))
write.csv(final, row.names = FALSE, "predictions.csv")
glmfin
glmfin <- h2o.glm(x = x, y = y,
training_frame = train, seed=1,
alpha = .25,
lambda = .001)
glmfin
glmfin <- h2o.glm(x = x, y = y,
training_frame = train, seed=1,
alpha = .35,
lambda = .001)
glmfin <- h2o.glm(x = x, y = y,
training_frame = train, seed=1,
alpha = .35,
lambda = .001)
glmfin
glmfin <- h2o.glm(x = x, y = y,
training_frame = train, seed=1,
alpha = .35,
lambda = .0015)
glmfin <- h2o.glm(x = x, y = y,
training_frame = train, seed=1,
alpha = .35,
lambda = .0015)
glmfin
pred <- exp(h2o.predict(object=glmfin, newdata=test))
pred <- exp(h2o.predict(object=glmfin, newdata=test))
pred
dftest <- read.csv(here::here("data-raw", "test.csv"))
dftest <- read.csv(here::here("data-raw", "test.csv"))
final <- data.frame(Id = dftest %>% dplyr::pull(Id), SalePrice = as.vector(pred))
write.csv(final, row.names = FALSE, "predictions.csv")
hyper_params <- list( lambda = seq(.001, .005, 0.0002),
alpha = seq(0,.5, .1))
y <- "SalePrice"
x <- setdiff(names(train), y)
# Grid search for selecting the best model
grid <- h2o.grid(x = x, y = y ,
training_frame = train,
fold_column = "fold",
algorithm = "glm", grid_id = "id6", hyper_params = hyper_params,seed=1,
search_criteria = list(strategy = "Cartesian"))
# For k_fold strategy we need to provide fold column
train$fold <- h2o.kfold_column(data = train, nfolds = 10, seed = seed)
y <- "SalePrice"
x <- setdiff(names(train), y)
# Grid search for selecting the best model
grid <- h2o.grid(x = x, y = y ,
training_frame = train,
fold_column = "fold",
algorithm = "glm", grid_id = "id6", hyper_params = hyper_params,seed=1,
search_criteria = list(strategy = "Cartesian"))
# Grid search for selecting the best model
grid <- h2o.grid(x = x, y = y ,
training_frame = train,
fold_column = "fold",
algorithm = "glm", grid_id = "id8", hyper_params = hyper_params,seed=1,
search_criteria = list(strategy = "Cartesian"))
sortedGrid <- h2o.getGrid("id8", sort_by = "rmse", decreasing = FALSE)
sortedGrid <- h2o.getGrid("id8", sort_by = "rmse", decreasing = FALSE)
sortedGrid
# For k_fold strategy we need to provide fold column
train$fold <- h2o.kfold_column(data = train, nfolds = 10, seed = seed, assignment_type = "Stratified")
# Grid search for selecting the best model
grid <- h2o.grid(x = x, y = y ,
training_frame = train,
nfolds = 10,
assignment_type = "Stratified",
algorithm = "glm", grid_id = "id8", hyper_params = hyper_params,seed=1,
search_criteria = list(strategy = "Cartesian"))
# Grid search for selecting the best model
grid <- h2o.grid(x = x, y = y ,
training_frame = train,
nfolds = 10,
assignment_type = "Stratified",
algorithm = "glm", grid_id = "id9", hyper_params = hyper_params,seed=1,
search_criteria = list(strategy = "Cartesian"))
# For k_fold strategy we need to provide fold column
train$fold <- h2o.kfold_column(data = train, nfolds = 10, seed = seed, fold_assignment = "Stratified")
# Grid search for selecting the best model
grid <- h2o.grid(x = x, y = y ,
training_frame = train,
nfolds = 10,
fold_assignment = "Stratified",
algorithm = "glm", grid_id = "id9", hyper_params = hyper_params,seed=1,
search_criteria = list(strategy = "Cartesian"))
grid
sortedGrid <- h2o.getGrid("id9", sort_by = "rmse", decreasing = FALSE)
sortedGrid <- h2o.getGrid("id9", sort_by = "rmse", decreasing = FALSE)
sortedGrid
library(magrittr)
df <- read.csv("dffinal.csv")
too_low_var <-
c(
"Condition2",
"Condition1",
"Alley",
"Fence",
"BsmtCond",
"Electrical",
"LandSlope",
"ExterCond",
"Functional",
"Neighborhood2")
# one hot encode
df <- df %>%
dplyr::select(-too_low_var)
# one hot encode
nomvars <- names(df)[sapply(df, class) %in% c('factor',"character")]
df2 <- df %>%
dplyr::mutate_at(nomvars, as.factor)%>%
dplyr::select(nomvars) %>%
dplyr::select(-set)
library(caret)
dummy <- caret::dummyVars(" ~ .", data=df2, fullRank = FALSE)
df2 <- data.frame(predict(dummy, newdata = df2))
dff <- cbind(df2, df %>% dplyr::select(-nomvars))
dff$set <- df$set
train <- dff %>%
dplyr::filter(set == "train") %>%
dplyr::select(-set)
test <- dff %>%
dplyr::filter(set == "test") %>%
dplyr::select(-set)
library(h2o)
h2o.init(nthreads= -1, max_mem_size = "8g")
train <- as.h2o(train)
test <- as.h2o(test)
seed <- 12345
# For k_fold strategy we need to provide fold column
# train$fold <- h2o.kfold_column(data = train, nfolds = 10, seed = seed, fold_assignment = "Stratified")
hyper_params <- list( lambda = seq(.001, .005, 0.0002),
alpha = seq(0,.5, .1))
y <- "SalePrice"
x <- setdiff(names(train), y)
# Grid search for selecting the best model
grid <- h2o.grid(x = x, y = y ,
training_frame = train,
nfolds = 10,
fold_assignment = "Stratified",
algorithm = "glm", grid_id = "id9", hyper_params = hyper_params,seed=1,
search_criteria = list(strategy = "Cartesian"))
grid
sortedGrid <- h2o.getGrid("id9", sort_by = "rmse", decreasing = FALSE)
sortedGrid
train$fold <- h2o.kfold_column(data = train, nfolds = 10, seed = seed)
hyper_params <- list( lambda = seq(.001, .005, 0.0002),
alpha = seq(0,.5, .1))
y <- "SalePrice"
x <- setdiff(names(train), y)
# Grid search for selecting the best model
grid <- h2o.grid(x = x, y = y ,
training_frame = train,
fold_column = "fold",
algorithm = "glm", grid_id = "id9", hyper_params = hyper_params,seed=1,
search_criteria = list(strategy = "Cartesian"))
grid
sortedGrid <- h2o.getGrid("id9", sort_by = "rmse", decreasing = FALSE)
sortedGrid
x <- setdiff(names(train), c(y, "fold"))
# Grid search for selecting the best model
grid <- h2o.grid(x = x, y = y ,
training_frame = train,
fold_column = "fold",
algorithm = "glm", grid_id = "id9", hyper_params = hyper_params,seed=1,
search_criteria = list(strategy = "Cartesian"))
# Grid search for selecting the best model
grid <- h2o.grid(x = x, y = y ,
training_frame = train,
fold_column = "fold",
algorithm = "glm", grid_id = "id9", hyper_params = hyper_params,seed=1,
search_criteria = list(strategy = "Cartesian"))
# Grid search for selecting the best model
grid <- h2o.grid(x = x, y = y ,
training_frame = train,
fold_column = "fold",
algorithm = "glm", grid_id = "id9", hyper_params = hyper_params,seed=1,
search_criteria = list(strategy = "Cartesian"))
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
# knitr::opts_knit$get("root.dir")  # alternative to the previous line
# the default autosave location will depend on this being setup
options(warn=-1)
library(magrittr)
df <- read.csv("dffinal.csv")
too_low_var <-
c(
"Condition2",
"Condition1",
"Alley",
"Fence",
"BsmtCond",
"Electrical",
"LandSlope",
"ExterCond",
"Functional",
"Neighborhood2")
# one hot encode
df <- df %>%
dplyr::select(-too_low_var)
# one hot encode
nomvars <- names(df)[sapply(df, class) %in% c('factor',"character")]
df2 <- df %>%
dplyr::mutate_at(nomvars, as.factor)%>%
dplyr::select(nomvars) %>%
dplyr::select(-set)
library(caret)
dummy <- caret::dummyVars(" ~ .", data=df2, fullRank = FALSE)
df2 <- data.frame(predict(dummy, newdata = df2))
dff <- cbind(df2, df %>% dplyr::select(-nomvars))
dff$set <- df$set
train <- dff %>%
dplyr::filter(set == "train") %>%
dplyr::select(-set)
test <- dff %>%
dplyr::filter(set == "test") %>%
dplyr::select(-set)
library(h2o)
h2o.init(nthreads= -1, max_mem_size = "8g")
train <- as.h2o(train)
test <- as.h2o(test)
seed <- 12345
# For k_fold strategy we need to provide fold column
train$fold <- h2o.kfold_column(data = train, nfolds = 10, seed = seed)
hyper_params <- list( lambda = seq(.001, .005, 0.0002),
alpha = seq(0,.5, .1))
y <- "SalePrice"
x <- setdiff(names(train), c(y, "fold"))
# Grid search for selecting the best model
grid <- h2o.grid(x = x, y = y ,
training_frame = train,
fold_column = "fold",
algorithm = "glm", grid_id = "id9", hyper_params = hyper_params,seed=1,
search_criteria = list(strategy = "Cartesian"))
grid
sortedGrid <- h2o.getGrid("id9", sort_by = "rmse", decreasing = FALSE)
sortedGrid
# Grid search for selecting the best model
grid <- h2o.grid(x = x, y = y ,
training_frame = train,
fold_column = "fold",
algorithm = "glm", grid_id = "id10", hyper_params = hyper_params,seed=1,
search_criteria = list(strategy = "Cartesian"))
sortedGrid <- h2o.getGrid("id10", sort_by = "rmse", decreasing = FALSE)
sortedGrid
glmfin <- h2o.glm(x = x, y = y,
training_frame = train, seed=1,
alpha = .5,
lambda = .005) #increasing lambda beyond grid to reduce predictors
glmfin
glmfin <- h2o.glm(x = x, y = y,
training_frame = train, seed=1,
alpha = .5,
lambda = .01) #increasing lambda beyond grid to reduce predictors
glmfin <- h2o.glm(x = x, y = y,
training_frame = train, seed=1,
alpha = .5,
lambda = .01) #increasing lambda beyond grid to reduce predictors
glmfin
pred <- exp(h2o.predict(object=glmfin, newdata=test))
pred <- exp(h2o.predict(object=glmfin, newdata=test))
pred
dftest <- read.csv(here::here("data-raw", "test.csv"))
dftest <- read.csv(here::here("data-raw", "test.csv"))
final <- data.frame(Id = dftest %>% dplyr::pull(Id), SalePrice = as.vector(pred))
